---
title: "DHARMa - Residual Diagnostics for HierArchical (Multi-level / Mixed) Regression Models"
author: "Florian Hartig, University of Freiburg, [website](https://florianhartig.wordpress.com/)"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Vignette for the DHARMa package}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8](inputenc)
---

```{r, echo = F}
library(DHARMa)
set.seed(123)
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=4.5, warning=FALSE, message=FALSE, cache = F)
```

***Summary: The DHARMa package creates readily interpretable residuals for generalized linear (mixed) models that are standardized to values between 0 and 1. This is achieved by a simulation-based approach, similar to the Bayesian p-value or the parametric bootstrap: 1) simulate new data from the fitted model 2) from this simulated data, calculate the cummulative density function  3) residual is the value of the empirical density function at the value of the observed data.*  **

# Motivation 

Residual interpretation for generalized linear mixed models is often problematic. As an example, here two Poisson models, one that is lacking a quadratic effect, and one that fits the data perfectly. I show three standard residuals diagnostics each. Can you say which is the misspecified model?


```{r, echo = F, fig.width=8, fig.height=3.5}
library(lme4)

overdispersedData = createData(sampleSize = 250, overdispersion = 0, quadraticFixedEffects = -2, family = poisson())
fittedModelOverdispersed <- glmer(observedResponse ~ Environment1 + (1|group) , family = "poisson", data = overdispersedData)

plotConventionalResiduals(fittedModelOverdispersed)


testData = createData(sampleSize = 250, intercept = 0, overdispersion = 0, family = poisson(), randomEffectVariance = 0)
fittedModel <- glmer(observedResponse ~ Environment1 + (1|group) , family = "poisson", data = testData)

plotConventionalResiduals(fittedModel)

```

Just for completeness - it was the first one. Don't get too excited if you got it right, though. Either you were simply lucky, or you noted that the first model seems a bit overdispersed. Even so, could you have said what the problem is? Would you have added a quadratic effect, instead of adding an overdispersion correction? The point that I want to make is that misspecifications in GL(M)Ms cannot reliably be diagnosed with standard residual plots. 

The reason is that in GL(M)Ms distribution shapes change with fitted values in all kind of ways. Simple reweightings as done in the Pearson residuals does not lead to visually homogenous residuals. Also deviance residuals don't solve that problem, because likelihood densities are equally non-homogenous, even in correctly specified models. As a result, standard residual plots, when interpreted in the same way as for linear models, seem to show all kind problems such as non-normality, heteroskedasticity and so on, even if the model is correctly specified, which regularly confuses statistical beginners. 

The more serious problem, however, is that even experienced statistical analysis have currently few options to diagnose problems in the specification of GLMMs. I would hold that the current standard practice is to eyeball residual plots for major misspecifications, and the run a test for overdispersion, which asks if the overall variation in the observations is larger or smaller than expected under the fitted model. This approach, however, has a number of problems. For example, heteroskedasticity cannot be reliably be diagnosed. 

DHARMa aims at solvind this problem by creating readily interpretable residuals for generalized linear (mixed) models that are standardized to values between 0 and 1. This is achieved by a simulation-based approach, similar to the Bayesian p-value or the parametric bootstrap.

# Workflow in DHARMa

## Installing, loading and citing the package

If you haven't installed the package yet, run

```{r, eval = F}
library(devtools)
install_url("https://dl.dropboxusercontent.com/s/xlvjf0vpslukl29/DHARMa.tar.gz", dependencies = T)
```

loading and citation

```{r}
set.seed(1)
library(DHARMa)
citation("DHARMa")
```

## Calculating and plotting scaled residuals 

The scaled residuals are calculated with the simulateResiduals() functions. You have to specify the number of simulations. The default number of simulations to run is 250, but for very stable results you may want to increase this number. 

```{r}
simulationOutput <- simulateResiduals(fittedModel = fittedModel, n = 250, refit = F, integerResponse = NULL)
```

What the function does is to create 250 new datasets from the fitted model, calculating the distribution of expected values for each observed value, and then returning the quantile value that would correspond to the observed value.

For example, a simulated residual of 0.5 means that half of the simulated values are higher, and half of them lower than the observed value. A value of 0.99 would mean that nearly all simulated values are lower than the observed value. The minium/maxium values for the residuals are 0 and 1. 

The calculated residuals are stored in the 

```{r, eval = F}
simulationOutput$scaledResiduals
```

The key idea for the interpretation of the residuals is that, if the model is correctly specified, then the observed data will basically look identical to be simulated data, and hence all residual values are equally likely. Hence, regardless of what the exact model structure is (Poisson, binomial, ...), for a correctly specified model we would expect 

* a uniform (flat) distribution of the overall residuals 

* uniformity in y direction if we plot against any predictor. 

We can get a visual impression of these properties with the plotSimulatedResiduals() function

```{r}
plotSimulatedResiduals(simulationOutput = simulationOutput)
```

which provides a qq-plot to detect overall deviations from normality, and a plot of the residuals against the predicted value. 

To provide a visual aid in detecting deviations from uniformity in y-direction, the plot of the residuals against the predited values also performs an (optional) quantile regression, which provides 0.25, 0.5 and 0.75 quantile lines across the plots. These lines should be straight, horizontal, and at y-values of 0.25, 0.5 and 0.75. 

If you want to plot the residuals against other predictors (highly recommend), you can use the function

```{r, eval = F}
plotResiduals(YOURPREDICTOR, simulationOutput$scaledResiduals)
```

which does the same quantile plot as the main plotting function. 

## Hypothesis tests in DHARMa

The DHARMa package provides a number of additional tests, which, however, should be viewed more as informal diagnostics than formal hypothesis tests. Likely, power and type-I error rates are rather low. If available, specialized parametric tests should be preferred. Nevertheless, the tests can be useful as confirmation of the visual analysis. 

### Omnibus test for deviations from overall uniformity 

You can run a hypothesis test on the residuals, which runs a KS test on the uniformity of the simulated residuals

```{r}
testUniformDistribution(simulationOutput = simulationOutput)
```

Note, however, that simulations show that this test is less powerfull than parametric tests on the likelihood that are currently run by many people. On the other hand, the parametric tests

```{r}
parametricDispersionTest(fittedModel, type = 1)
```

### Test for zero-inflation

There is a special test for zero-inflation, which compares the distribution of expected zeros in the data against the observed zeros

```{r, fig.width=4, fig.height=4.5}
testZeroInflation(simulationOutput)
```


### Test for temporal autocorrelation

The temporal autocorrelation test plots and test the temporal autocorrelation of the data 

```{r, fig.width=4, fig.height=4.5}
testTemporalAutocorrelation(simulationOutput, time = runif(length(simulationOutput$observedResponse )))
```


### Test for spatial autocorrelation

The spatial autocorrelation test plots and test the spatial autocorrelation of the data 

```{r, fig.width=4, fig.height=4.5}
testSpatialAutocorrelation(simulationOutput, x = runif(length(simulationOutput$observedResponse )), y =  runif(length(simulationOutput$observedResponse )))
```


# Diagnosing problems in the residuals 

So far, all the plots / tests were run for a correctly specified model. In this section, we discuss how model misspecification will show up in the scaled residuals.

## Overdispersion / underdispersion / zero-inflation

The most common concern for GLMMs is overdispersion, underdispersion and zero-inflation.

Over/underdispersion refers to the phenomenon that that residual variance is larger/smaller than expected under the fitted model. Over/underdispersion can appear for any distributional family with fixed variance, in particular for Poisson and binomial models. 

A common special case of overdispersion is zero-inflation, which is the situation when more zeros appear in the observation than expected under the fitted model. Zero-inflation requires special correction steps. 

A few general rules of thumb

* You can detect overdispersion / zero-inflation only AFTER fitting the model
* Overdispersion is more common than underdispersion
* If overdispersion is present, confidence intervals tend to be too narrow, and p-values to small. The opposite is true for underdispersion
* A common reason for overdispersion is a misspecified models. When overdispersion is detected, one should therefore firest search for problems in the model specification (e.g. by plotting residuals against predictors), and only if this doesn't lead to success, overdispersion corrections such as individual-level random effects or changes in the distribution should be applied

#### An example of overdispersion

This this is how **overdispersion** looks like in the DHARMa residuals

```{r}
testData = createData(sampleSize = 500, overdispersion = 2, family = poisson())
fittedModel <- glmer(observedResponse ~ Environment1 + (1|group) , family = "poisson", data = testData)

simulationOutput <- simulateResiduals(fittedModel = fittedModel)
plotSimulatedResiduals(simulationOutput = simulationOutput)
```

Note that we get more residuals around 0 and 1, which means that residuals are in the tail of distribution that would be expected under the fitted model. 

#### An example of underdispersion

And this is underdispersion 

```{r}
testData = createData(sampleSize = 500, intercept=0, fixedEffects = 2, overdispersion = 0, family = poisson(), roundPoissonVariance = 0.001, randomEffectVariance = 0)
fittedModel <- glmer(observedResponse ~ Environment1 + (1|group) , family = "poisson", data = testData)

summary(fittedModel)

# plotConventionalResiduals(fittedModel)

simulationOutput <- simulateResiduals(fittedModel = fittedModel)
plotSimulatedResiduals(simulationOutput = simulationOutput)
testUniformDistribution(simulationOutput = simulationOutput)
```

Here, we get too many residuals around 0.5, which means that we are not getting as many residuals as we would expect in the tail of the distribution that is epected with the fitted model. 

#### An example of zero-inflation

For pedagogical reasons, I plot this data against the environmental predictor

```{r}
testData = createData(sampleSize = 500, intercept = 2, fixedEffects = c(1), overdispersion = 0, family = poisson(), quadraticFixedEffects = c(-3), randomEffectVariance = 0, pZeroInflation = 0.6)

par(mfrow = c(1,2))
plot(testData$Environment1, testData$observedResponse, xlab = "Envrionmental Predictor", ylab = "Response")
hist(testData$observedResponse, xlab = "Response", main = "")
```

We see a hump-shaped dependence of the environment, but with too many zeros. In the residuals, this looks pretty much like overdispersion

```{r}

fittedModel <- glmer(observedResponse ~ Environment1 + I(Environment1^2) + (1|group) , family = "poisson", data = testData)

simulationOutput <- simulateResiduals(fittedModel = fittedModel)
plotSimulatedResiduals(simulationOutput = simulationOutput)
```

The reason is that the model will usually try to find a compromise between the zeros, and the other values, which will lead to excecss variation in the resiudals. The overdispersion can be better detected with this specialized plot / test

```{r, fig.width=4, fig.height=4.5}
testZeroInflation(simulationOutput)
```

which shows the expected distribution of the number of zeros against the observed number of zeros. 

## Heteroscedasticity

So far, most of the things that we have tested could also have been detected with paramtric tests. Here, we come to the first issue that is difficult to detect with current tests, and that is usually neglected. 

Heteroscedasticity means that there is a systematic dependency of the dispersion / variance on another variable in the model. It is not sufficiently appreciated that also binomial or poisson models can show heteroscedasticity. Basically, it means that the level of over/underdispersion depends on another parameter. Here an example where we create such data 

```{r}
testData = createData(sampleSize = 500, intercept = 0, overdispersion = function(x){return(rnorm(length(x), sd = 2*abs(x)))}, family = poisson(), randomEffectVariance = 0)
fittedModel <- glmer(observedResponse ~ Environment1 + (1|group), family = "poisson", data = testData)

simulationOutput <- simulateResiduals(fittedModel = fittedModel)
plotSimulatedResiduals(simulationOutput = simulationOutput)
testUniformDistribution(simulationOutput = simulationOutput)
```

Adding a simple overdispersion correction will try to find a compromise between the different levels of dispersion in the model. The qq plot looks better now, but there is still a pattern in the residuals 

```{r}
testData = createData(sampleSize = 500, intercept = 0, overdispersion = function(x){return(rnorm(length(x), sd = 2*abs(x)))}, family = poisson(), randomEffectVariance = 0)
fittedModel <- glmer(observedResponse ~ Environment1 + (1|group) + (1|ID), family = "poisson", data = testData)

# plotConventionalResiduals(fittedModel)

simulationOutput <- simulateResiduals(fittedModel = fittedModel)
plotSimulatedResiduals(simulationOutput = simulationOutput)
testUniformDistribution(simulationOutput = simulationOutput)
```

To remove this patter, you would need to make the dispersion parameter dependent on a predictor (e.g. in JAGS), or apply a transformation. 

## Missing predictors or quadratic effects 

A second test that is typically run for LMs, but not for GL(M)Ms is to plot residuals against the predictors in the model (or potentially predictors that were not in the model) to detect possible misspecifcations. Doing this is *highly recommeded*. For that purpose, you can retrieve the residuals via 

```{r, eval = F}
simulationOutput$scaledResiduals
```

Note again that the residual values are scaled between 0 and 1. If you plot the residuals against predictors, space or time, the resulting plots should not only show no systematic dependency of those residuals on the covariates, but they should also again be flat for each fixed situation. That means that if you have, for example, a categorical predictor: treatment / control, the distribution of residuals for each predictor alone should be flat as well. 

Here an example with a missing quadratic effect in the model and 2 predictors

```{r}
testData = createData(sampleSize = 200, intercept = 1, fixedEffects = c(1,2), overdispersion = 0, family = poisson(), quadraticFixedEffects = c(-3,0))
fittedModel <- glmer(observedResponse ~ Environment1 + Environment2 + (1|group) , family = "poisson", data = testData)
simulationOutput <- simulateResiduals(fittedModel = fittedModel)
# plotConventionalResiduals(fittedModel)
plotSimulatedResiduals(simulationOutput = simulationOutput, quantreg = T)
testUniformDistribution(simulationOutput = simulationOutput)
```

Difficult to see with the overall pattern, but it becomes clear if we plot against the environment

```{r}
par(mfrow = c(1,2))
plotResiduals(testData$Environment1,  simulationOutput$scaledResiduals)
plotResiduals(testData$Environment2,  simulationOutput$scaledResiduals)
```

## Spatial and temporal autocorrelation

A special case of the plotting residuals against predictors is the plot against time and space, which should always be performed if those variables are present in the model. This can be done by hand, but is suppored by the previously introduced tests 

```{r, fig.width=4, fig.height=4, eval = F}
testTemporalAutocorrelation(simulationOutput, time = runif(length(simulationOutput$observedResponse )))
testSpatialAutocorrelation(simulationOutput, x = runif(length(simulationOutput$observedResponse )), y =  runif(length(simulationOutput$observedResponse )))
```

# Real-world examples

## Budworm example

This example comes from Jochen Fründ. Measured are the number of parasitized adults against the density

```{r, echo = F}
data = structure(list(N_parasitized = c(226, 689, 481, 960, 1177, 266, 
46, 4, 884, 310, 19, 4, 7, 1, 3, 0, 365, 388, 369, 829, 532, 
5), N_adult = c(1415, 2227, 2854, 3699, 2094, 376, 8, 1, 1379, 
323, 2, 2, 11, 2, 0, 1, 1394, 1392, 1138, 719, 685, 3), density.attack = c(216.461273226486, 
214.662143448767, 251.881252132684, 400.993643475831, 207.897856251888, 
57.0335141562012, 6.1642552100285, 0.503930659141302, 124.673812637575, 
27.3764667492035, 0.923453215863429, 0.399890030241684, 0.829818131526174, 
0.146640466903247, 0.216795117773948, 0.215498663908284, 110.635445098884, 
91.3766566822467, 126.157080458047, 82.9699108890686, 61.0476207779938, 
0.574539291305784), Plot = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L
), .Label = c("1", "2", "3", "4"), class = "factor"), PY = c("p1y82", 
"p1y83", "p1y84", "p1y85", "p1y86", "p1y87", "p1y88", "p1y89", 
"p2y86", "p2y87", "p2y88", "p2y89", "p2y90", "p2y91", "p2y92", 
"p2y93", "p3y88", "p3y89", "p3y90", "p3y91", "p3y92", "p3y93"
), Year = c(82, 83, 84, 85, 86, 87, 88, 89, 86, 87, 88, 89, 90, 
91, 92, 93, 88, 89, 90, 91, 92, 93), ID = 1:22), .Names = c("N_parasitized", 
"N_adult", "density.attack", "Plot", "PY", "Year", "ID"), row.names = c("p1y82", 
"p1y83", "p1y84", "p1y85", "p1y86", "p1y87", "p1y88", "p1y89", 
"p2y86", "p2y87", "p2y88", "p2y89", "p2y90", "p2y91", "p2y92", 
"p2y93", "p3y88", "p3y89", "p3y90", "p3y91", "p3y92", "p3y93"
), class = "data.frame")
```


```{r, fig.width=4, fig.height=4.5}
plot(data$N_parasitized / (data$N_adult + data$N_parasitized ) ~ log10(data$density.attack + 1), xlab = "Density", ylap = "Proportion infected" )
```

Fitting the data with a regular Poisson model

```{r}
mod1 <- glm(cbind(N_parasitized, N_adult) ~ log10(density.attack+1), data = data, family=binomial)
simulationOutput <- simulateResiduals(fittedModel = mod1)
plotSimulatedResiduals(simulationOutput = simulationOutput)
```

The residuals look clearly overdispersed. We can confirm that with a 

```{r}
testUniformDistribution(simulationOutput = simulationOutput)
```


```{r}
mod2 <- glmer(cbind(N_parasitized, N_adult) ~ log10(density.attack+1) + (1|ID), data = data, family=binomial)
simulationOutput <- simulateResiduals(fittedModel = mod2)
plotSimulatedResiduals(simulationOutput = simulationOutput)

```

Overdispersion looks better, but you can see that the residuals still detect an issue with the steep inclrease in the beginning that one an see in the raw data. One would probably need to apply another transformation or a nonlinear function to completely fit this away.



